{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Keks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Keks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from sqlalchemy import text\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy\n",
    "import numpy as np\n",
    "import swifter\n",
    "from pandarallel import pandarallel\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "stop_words = set(stopwords.words('english') + stopwords.words('german') + stopwords.words('french') + stopwords.words('spanish'))\n",
    "nltk.download('wordnet')\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql+psycopg2://postgres:4750@localhost:5432/ba1')\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swifter import set_defaults\n",
    "set_defaults(\n",
    "    npartitions=8,\n",
    "    dask_threshold=1,\n",
    "    scheduler=\"processes\",\n",
    "    progress_bar=True,\n",
    "    progress_bar_desc=None,\n",
    "    allow_dask_on_strings=True,\n",
    "    force_parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(sqlalchemy.text(\"\"\"\n",
    "SELECT\n",
    "dbrecordid,  \n",
    "collectiontitle,\n",
    "abstract,\n",
    "title\n",
    "FROM  ke_stage.bachelor_mp_raw_ke\n",
    "LIMIT 1000;\"\"\"),\n",
    "engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "collist= [\n",
    "\"collectiontitle\",\n",
    "\"abstract\",\n",
    "\"title\"\n",
    "]\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords\n",
    "for column in df:\n",
    "    print(column)\n",
    "    df[column] = df[column].astype(str)\n",
    "    try:\n",
    "        df[column] = df[column].progress_apply(lambda x: re.sub('(^\\{\\\")|(\\\"\\}$)|(^\\{)|(\\}$)', '', x))\n",
    "    except:\n",
    "        print(\"fehler,weitermachen\")\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Keks\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\distributed\\node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 49907 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "client = Client()\n",
    "# create a dask dataframe\n",
    "ddf = dd.from_pandas(df, npartitions=6)\n",
    "for column in ddf:\n",
    "    print(column)\n",
    "    if column in collist:\n",
    "        ddf[column] = ddf[column].map(lambda x: x.lower())\n",
    "        ddf[column+\"_token\"] = ddf[column].apply(lambda x: [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(x)], meta=('column', 'object'))\n",
    "        ddf[column+\"_token\"] = ddf[column+\"_token\"].apply(lambda x: gensim.utils.simple_preprocess(str(x), deacc=True), meta=('column', 'object'))\n",
    "        ddf[column+\"_token\"] = ddf[column+\"_token\"].apply(lambda x: ','.join([word for word in x if word not in (stop_words)]), meta=('column', 'object'))\n",
    "\n",
    "# drop unnecessary columns\n",
    "ddf = ddf.drop(columns=[\"abstract\",\"title\",\"collectiontitle\"])\n",
    "\n",
    "# rename column\n",
    "ddf = ddf.rename(columns={\"dbrecordid\": \"token_dbrecordid\"})\n",
    "\n",
    "# convert back to pandas dataframe\n",
    "df = ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = create_engine('postgresql+psycopg2://postgres:4750@localhost:5432/ba1')\n",
    "df.to_sql(\"db_tokenized\", engine,schema=\"ke_stage\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inner Join in PGAdmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca37fc2f56e054c56d59296cb3017116b9862e7f9ad082ff8860f3927f933d67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
