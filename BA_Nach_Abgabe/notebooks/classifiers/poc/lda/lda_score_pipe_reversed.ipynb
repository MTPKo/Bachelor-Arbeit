{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sqlalchemy\n",
    "from sqlalchemy import text\n",
    "from pandarallel import pandarallel\n",
    "import regex as re\n",
    "pandarallel.initialize(progress_bar=False, nb_workers= 8)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "from pyLDAvis import gensim_models\n",
    "import gensim.corpora as corpora\n",
    "import pyLDAvis.gensim_models\n",
    "import os\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql+psycopg2://postgres:4750@192.168.0.137:5432/ba1')\n",
    "\n",
    "def sql_read(topics,lim):\n",
    "    stat= \"SELECT record_id, collectiontitle_token,abstract_token,title_token,class FROM  ke_stage.ba_corpus_2 WHERE class LIKE \"+str(topics)+\" LIMIT \" +str(lim)\n",
    "    df = pd.read_sql_query(sqlalchemy.text(str(stat)),engine)\n",
    "    return df\n",
    "\n",
    "def to_data(df):\n",
    "    data=[]\n",
    "    for row in tqdm(df['combined'].values):\n",
    "        row = row.split(\",\")\n",
    "        data.append(row)\n",
    "    return data\n",
    "def to_id_corpus(data):\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data)\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in data]\n",
    "    return corpus, id2word\n",
    "\n",
    "def row2data(row):\n",
    "    data=[]\n",
    "    row = row.split(\",\")\n",
    "    data.append(row)\n",
    "    return data\n",
    "\n",
    "def get_topic(row ,lda):\n",
    "    to_pro = []\n",
    "    data = row2data(row)\n",
    "    corpus, id2w = to_id_corpus(data)\n",
    "    \n",
    "    topic = lda.get_document_topics(corpus, minimum_probability=0.5, minimum_phi_value=None,\n",
    "                                   per_word_topics=False)\n",
    "    for t in topic:\n",
    "            to_pro.append(t)\n",
    "    return(to_pro)\n",
    "\n",
    "def count_class_pop(df):\n",
    "    counted=df['class'].value_counts()\n",
    "    counted = counted.to_frame()\n",
    "    counted[\"population\"] = counted['class'].values / len(df)\n",
    "    counted[\"pop_perc\"] = counted['population'].values * 100\n",
    "    #ax = sns.barplot(data=counted,x=\"pop_perc\",  y= counted.index, orient=\"h\")\n",
    "    #ax.set(xlabel=\"Anteil der Publikation in %\",ylabel=\"Klasse\")\n",
    "    #plt.show()\n",
    "    lowest_c = counted.min()['class']\n",
    "    return counted , lowest_c\n",
    "\n",
    "def combine_tokens(df):\n",
    "    df['combined'] = df[df.columns[1:3]].parallel_apply(lambda x: ','.join(x.astype(str)),axis=1)\n",
    "    df = df.drop(['title_token',\n",
    "                'abstract_token','collectiontitle_token'\n",
    "                ],axis =1 )\n",
    "    df = df[df[\"combined\"].str.len() > 3]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  100  Einträge aus der Datenbank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<?, ?it/s]\n",
      "100%|██████████| 7/7 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainiere LDA Model mit  4  Topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 6990.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  400  Einträge aus der Datenbank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 199728.76it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainiere LDA Model mit  4  Topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 14293.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  1600  Einträge aus der Datenbank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599/1599 [00:00<00:00, 200421.12it/s]\n",
      "100%|██████████| 533/533 [00:00<00:00, 264469.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainiere LDA Model mit  4  Topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 533/533 [00:00<00:00, 14235.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  6400  Einträge aus der Datenbank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6471/6471 [00:00<00:00, 149602.54it/s]\n",
      "100%|██████████| 2157/2157 [00:00<00:00, 190802.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainiere LDA Model mit  4  Topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2157/2157 [00:00<00:00, 12113.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  25600  Einträge aus der Datenbank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26400/26400 [00:00<00:00, 137630.46it/s]\n",
      "100%|██████████| 8800/8800 [00:00<00:00, 131552.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainiere LDA Model mit  4  Topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8800/8800 [00:00<00:00, 10867.43it/s]\n"
     ]
    }
   ],
   "source": [
    "max_df_size = 50000\n",
    "min_df_size = 100\n",
    "\n",
    "modi = 4\n",
    "\n",
    "while min_df_size < max_df_size:\n",
    "    lim=min_df_size    \n",
    "    print(\"loading \", int(lim),\" Einträge aus der Datenbank\")\n",
    "    df_med = sql_read(\"'Medizin'\",lim)                 \n",
    "    df_land = sql_read(\"'Landwirtschaft'\",lim)          \n",
    "    df_umwelt = sql_read(\"'Umweltwissenschaften'\",lim)     \n",
    "    df_ern = sql_read(\"'ErnÃ¤hrung'\",lim)     \n",
    "    df = pd.concat([df_med, df_land,df_umwelt,df_ern])\n",
    "    df = df.replace('', np.nan)\n",
    "    df = df.drop(df[pd.isna(df['collectiontitle_token']) & pd.isna(df['abstract_token']) & pd.isna(df['title_token'])].index)\n",
    "    df = df.replace(np.nan,'')\n",
    "    df = combine_tokens(df)\n",
    "\n",
    "    min_df_size = int(min_df_size)\n",
    "    df_med = df.loc[df['class'] == \"Medizin\"].head(int(min_df_size))\n",
    "    df_land = df.loc[df['class'] =='Landwirtschaft'].head(int(min_df_size))         \n",
    "    df_umwelt = df.loc[df['class'] =='Umweltwissenschaften'].head(int(min_df_size))  \n",
    "    df_ern = df.loc[df['class'] =='ErnÃ¤hrung'].head(int(min_df_size))            \n",
    "    df = pd.concat([df_med, df_land,df_umwelt,df_ern])\n",
    "    counted, lowest_c = count_class_pop(df)\n",
    "    df_med = df.loc[df['class'] == \"Medizin\"].head(int(lowest_c))\n",
    "    df_land = df.loc[df['class'] =='Landwirtschaft'].head(int(lowest_c))         \n",
    "    df_umwelt = df.loc[df['class'] =='Umweltwissenschaften'].head(int(lowest_c))  \n",
    "    df_ern = df.loc[df['class'] =='ErnÃ¤hrung'].head(int(lowest_c))            \n",
    "    df = pd.concat([df_med, df_land,df_umwelt,df_ern])\n",
    "    counted, lowest_c = count_class_pop(df)\n",
    "\n",
    "    df_train, df_test = train_test_split(df, test_size=0.25)\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    df_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train_data = to_data(df_train)\n",
    "    test_data = to_data(df_test)\n",
    "    train_corpus, train_id2w = to_id_corpus(train_data)\n",
    "    test_corpus, test_id2w = to_id_corpus(test_data)\n",
    "    num_topics = 4\n",
    "    while num_topics < 5:\n",
    "        print(\"trainiere LDA Model mit \", int(num_topics),\" Topics\")    \n",
    "        lda = gensim.models.ldamulticore.LdaMulticore(\n",
    "                                corpus=train_corpus,\n",
    "                                num_topics=num_topics,\n",
    "                                id2word=train_id2w,\n",
    "                                chunksize=1000,\n",
    "                                workers=10, # Num. Processing Cores - 1\n",
    "                                passes=30,\n",
    "                                eval_every = 6,\n",
    "                                per_word_topics=False)\n",
    "        df_test[\"topic\"] = df_test[\"combined\"].apply(lambda x: get_topic(x , lda))\n",
    "        df_test[\"topic\"] = df_test[\"topic\"].astype(\"str\")\n",
    "        df_test[\"topic\"] = df_test[\"topic\"].replace(to_replace=r'[^\\d|\\.|\\,]', value='', regex=True)\n",
    "        df_test[\"topic\"] = df_test[\"topic\"].replace('', np.nan)\n",
    "        df_test[[\"topic\",\"certainty\"]] =  df_test[\"topic\"].progress_apply(lambda x: pd.Series(str(x).split(\",\")))\n",
    "        df_test.to_csv(\"scores_csv\\\\Klasse_zu_pub_\"+str(min_df_size)+\"_topics_\"+str(num_topics)+\".csv\")\n",
    "        df_topic_terms = pd.DataFrame(lda.print_topics())\n",
    "        df_topic_terms.to_csv(\"scores_csv\\\\terms_zu_pub_\"+str(min_df_size)+\"_topics_\"+str(num_topics)+\".csv\")\n",
    "        #count_series =  df_test.groupby(['class','topic'])['class'].count()\n",
    "        #count_series = count_series.to_frame()\n",
    "        #count_series.columns = ['count']\n",
    "        #count_series = count_series.reset_index()\n",
    "        #r_classes = df_test['class'].value_counts()\n",
    "        #print(\"Anzahl der Publiktationen pro Klasse:\" ,int(lowest_c), \" und \", int( num_topics),\" Topics\")\n",
    "        #ax = sns.barplot(data=count_series,x=\"count\",  y=\"class\", orient=\"h\", hue=\"topic\")\n",
    "        #ax.set(xlabel=\"Menge an Zugewiesenen Topics pro Klasse\",ylabel=\"Klasse(Averbis) & Topic(LDA)\")\n",
    "        #ax.text(x=0.5, y=1.1, s='Klassifzierungsgenauigkeit des LDA Models anhand des Testdatensatzes',\n",
    "        #        fontsize=13, weight='bold',\n",
    "        #        ha='center', va='bottom', transform=ax.transAxes)\n",
    "        #ax.text(x=0.5, y=1.05, s=\"bei einer Testdatensatzgröße von \" + str(len(df_test))+ \" Publikationen und \"+ str(num_topics)+' \"Topics\"',\n",
    "        #        fontsize=8, alpha=0.75, ha='center', va='bottom', transform=ax.transAxes)\n",
    "       # \n",
    "        #fig = ax.get_figure()\n",
    "        #fig.savefig(\"grafiken\\\\Klasse_zu_pub_\"+str(min_df_size)+\"_topics_\"+str(num_topics)+\".png\",dpi=300, bbox_inches = \"tight\") \n",
    "        #fig.clf()\n",
    "        #r_classes.to_csv(\"lda_score_csv_test\\\\classes_at_\"+str(min_df_size)+\"_topics_\"+str(num_topics)+\".csv\")\n",
    "        num_topics = num_topics + 4\n",
    "\n",
    "    min_df_size = min_df_size*modi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"\" + 0.006*\"water\" + 0.006*\"soil\" + 0.006*\"specie\" + 0.005*\"wa\" + 0.004*\"plant\" + 0.004*\"area\" + 0.003*\"study\" + 0.003*\"model\" + 0.003*\"result\"'),\n",
       " (1,\n",
       "  '0.016*\"wa\" + 0.011*\"patient\" + 0.009*\"study\" + 0.008*\"\" + 0.004*\"risk\" + 0.004*\"group\" + 0.004*\"xa\" + 0.004*\"disease\" + 0.004*\"data\" + 0.003*\"treatment\"'),\n",
       " (2,\n",
       "  '0.016*\"cell\" + 0.011*\"gene\" + 0.009*\"\" + 0.008*\"expression\" + 0.007*\"protein\" + 0.005*\"dna\" + 0.004*\"wa\" + 0.004*\"cells\" + 0.004*\"sequence\" + 0.003*\"cancer\"'),\n",
       " (3,\n",
       "  '0.020*\"wa\" + 0.009*\"\" + 0.006*\"effect\" + 0.005*\"concentration\" + 0.005*\"study\" + 0.004*\"xa\" + 0.004*\"acid\" + 0.004*\"activity\" + 0.004*\"level\" + 0.004*\"result\"')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
